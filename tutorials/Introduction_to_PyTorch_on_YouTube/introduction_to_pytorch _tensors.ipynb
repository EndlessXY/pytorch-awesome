{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch Tensors PyTorch 张量简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请观看下面或[YouTube](https://www.youtube.com/watch?v=r7QDUPb2dCM)上的视频。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量是 PyTorch 中的核心数据抽象。这个notebook深入介绍了torch.Tensor类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，让我们导入 PyTorch 模块。我们还将添加 Python 的数学模块来简化一些示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tensors 创建张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建张量的最简单方法是使用`torch.empty()`调用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(3, 4)\n",
    "print(type(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们解开我们刚刚所做的事情：\n",
    "- 我们使用`torch`模块附带的众多方法之一创建了一个张量。\n",
    "- 张量本身是二维的，有 3 行和 4 列。\n",
    "- 返回对象的类型是`torch.Tensor` ，它是`torch.FloatTensor`的别名；默认情况下，PyTorch 张量由 `32` 位浮点数填充。\n",
    "- 打印张量时，您可能会看到一些看起来随机的值。 `torch.empty()`调用为张量分配内存，但不使用任何值对其进行初始化 - 因此您看到的是分配时内存中的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于张量及其维数和术语的简要说明：\n",
    "- 有时您会看到称为向量的一维张量。\n",
    "- 同样，二维张量通常称为矩阵。\n",
    "- 任何超过二维的东西通常都被称为张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常，您需要使用某个值来初始化张量。常见情况是全零、全一或随机值，并且torch模块为所有这些提供了方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.zeros(2, 3)\n",
    "print(zeros)\n",
    "\n",
    "ones = torch.ones(2, 3)\n",
    "print(ones)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random = torch.rand(2, 3)\n",
    "print(random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有一个充满 0 的张量，另一个充满 1 的张量，另一个充满 0 到 1 之间的随机值的张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Tensors and Seeding 随机张量和种子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说到随机张量，您是否注意到紧随其前的`torch.manual_seed()`调用？使用随机值初始化张量（例如模型的学习权重）很常见，但有时（尤其是在研究环境中）您需要确保结果的可重复性。手动设置随机数生成器的种子是实现此目的的方法。让我们更仔细地看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "应该在上面看到的是random1和random3具有相同的值， random2和random4也是如此。手动设置 RNG 的种子会重置它，因此在大多数设置中，取决于随机数的相同计算应该提供相同的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有关更多信息，请参阅[有关重现性的 PyTorch 文档](https://pytorch.org/docs/stable/notes/randomness.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Shapes 张量形状"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常，当您对两个或多个张量执行操作时，它们需要具有相同的形状- 即具有相同的维度数以及每个维度中相同的单元数。为此，我们有`torch.*_like()`方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0.6128, 0.1519, 0.0453],\n",
      "         [0.5035, 0.9978, 0.3884]],\n",
      "\n",
      "        [[0.6929, 0.1703, 0.1384],\n",
      "         [0.4759, 0.7481, 0.0361]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(2, 2, 3)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "empty_like_x = torch.empty_like(x)\n",
    "print(empty_like_x.shape)\n",
    "print(empty_like_x)\n",
    "\n",
    "zeros_like_x = torch.zeros_like(x)\n",
    "print(zeros_like_x.shape)\n",
    "print(zeros_like_x)\n",
    "\n",
    "ones_like_x = torch.ones_like(x)\n",
    "print(ones_like_x.shape)\n",
    "print(ones_like_x)\n",
    "\n",
    "rand_like_x = torch.rand_like(x)\n",
    "print(rand_like_x.shape)\n",
    "print(rand_like_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面代码单元中的第一个新内容是在张量上使用`.shape`属性。此属性包含张量每个维度的范围的列表 - 在我们的例子中， x是形状为 `2 x 2 x 3` 的三维张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，我们调用`.empty_like()` 、 `.zeros_like()` 、 `.ones_like()`和`.rand_like()`方法。使用.shape属性，我们可以验证这些方法中的每一个都返回具有相同维度和范围的张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建将覆盖的张量的最后一种方法是直接从 PyTorch 集合指定其数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1416, 2.7183],\n",
      "        [1.6180, 0.0073]])\n",
      "tensor([ 2,  3,  5,  7, 11, 13, 17, 19])\n",
      "tensor([[2, 4, 6],\n",
      "        [3, 6, 9]])\n"
     ]
    }
   ],
   "source": [
    "some_constants = torch.tensor([[3.1415926, 2.71828], [1.61803, 0.0072897]])\n",
    "print(some_constants)\n",
    "\n",
    "some_integers = torch.tensor((2, 3, 5, 7, 11, 13, 17, 19))\n",
    "print(some_integers)\n",
    "\n",
    "more_integers = torch.tensor(((2, 4, 6), [3, 6, 9]))\n",
    "print(more_integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果 Python 元组或列表中已有数据，则使用`torch.tensor()`是创建张量的最直接方法。如上所示，嵌套集合将产生多维张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### NOTE\n",
    "> `torch.tensor()`创建数据的副本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Data Types 张量数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有几种方法可以设置张量的数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[ 0.9956,  1.4148,  5.8364],\n",
      "        [11.2406, 11.2083, 11.6692]], dtype=torch.float64)\n",
      "tensor([[ 0,  1,  5],\n",
      "        [11, 11, 11]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(a)\n",
    "\n",
    "b = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
    "print(b)\n",
    "\n",
    "c = b.to(torch.int32)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置张量基础数据类型的最简单方法是在创建时使用可选参数。在上面单元格的第一行中，我们为张量`a`设置`dtype=torch.int16` 。当我们打印`a`时，我们可以看到它充满了`1`而不是`1.` - Python 提示是这是一个整数类型而不是浮点数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于打印`a`需要注意的另一件事是，与我们将`dtype`保留为默认值（32 位浮点）不同，打印张量还指定其`dtype` 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可能还发现，我们从将张量的形状指定为一系列整数参数，到将这些参数分组在一个元组中。这并不是绝对必要的 - PyTorch 会将一系列初始的、未标记的整数参数作为张量形状 - 但在添加可选参数时，它可以使您的意图更具可读性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置数据类型的另一种方法是使用`.to()`方法。在上面的单元格中，我们以通常的方式创建一个随机浮点张量`b` 。接下来，我们通过使用`.to()`方法将`b`转换为 `32` 位整数来创建`c` 。请注意， `c`包含与`b`相同的所有值，但被截断为整数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有关详细信息，请参阅数[据类型文档](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math & Logic with PyTorch Tensors 使用 PyTorch 张量进行数学与逻辑运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们首先看看基本算术，以及张量如何与简单标量交互："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n",
      "tensor([[1.4142, 1.4142],\n",
      "        [1.4142, 1.4142]])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.zeros(2, 2) + 1\n",
    "twos = torch.ones(2, 2) * 2\n",
    "threes = (torch.ones(2, 2) * 7 - 1) / 2\n",
    "fours = twos ** 2\n",
    "sqrt2s = twos ** 0.5\n",
    "\n",
    "print(ones)\n",
    "print(twos)\n",
    "print(threes)\n",
    "print(fours)\n",
    "print(sqrt2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上所示，张量和标量之间的算术运算（例如加法、减法、乘法、除法和求幂）分布在张量的每个元素上。由于此类运算的输出将是一个张量，因此您可以使用通常的运算符优先级规则将它们链接在一起，就像我们创建`threes`的行一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两个张量之间的类似操作也像您直观地期望的那样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  4.],\n",
      "        [ 8., 16.]])\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.]])\n",
      "tensor([[12., 12.],\n",
      "        [12., 12.]])\n"
     ]
    }
   ],
   "source": [
    "powers2 = twos ** torch.tensor([[1, 2], [3, 4]])\n",
    "print(powers2)\n",
    "\n",
    "fives = ones + fours\n",
    "print(fives)\n",
    "\n",
    "dozens = threes * fours\n",
    "print(dozens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里需要注意的是，前面的代码单元中的所有张量都具有相同的形状。当我们尝试对形状不同的张量执行二元运算时会发生什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下单元格抛出运行时错误"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### NOTE\n",
    "> 以下单元格抛出运行时错误\n",
    "> ```python\n",
    "> a = torch.rand(2, 3)\n",
    "> b = torch.rand(3, 2)\n",
    "> print(a * b)\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在一般情况下，您不能以这种方式对不同形状的张量进行操作，即使在像上面的单元格这样的情况下，其中张量具有相同数量的元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Brief: Tensor Broadcasting 简介：张量广播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### NOTE\n",
    "> 如果您熟悉 NumPy ndarray 中的广播语义，您会发现此处适用相同的规则。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是一个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6146, 0.5999, 0.5013, 0.9397],\n",
      "        [0.8656, 0.5207, 0.6865, 0.3614]])\n",
      "tensor([[1.2291, 1.1998, 1.0026, 1.8793],\n",
      "        [1.7312, 1.0413, 1.3730, 0.7228]])\n"
     ]
    }
   ],
   "source": [
    "rand = torch.rand(2, 4)\n",
    "doubled = rand * (torch.ones(1, 4) * 2)\n",
    "\n",
    "print(rand)\n",
    "print(doubled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里有什么技巧呢？我们如何将 2x4 张量乘以 1x4 张量？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "广播是一种在形状相似的张量之间执行操作的方法。在上面的示例中，一行四列张量乘以两行四列张量的两行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是深度学习中的一个重要操作。常见的示例是将学习权重张量乘以一批输入张量，分别将运算应用于批次中的每个实例，并返回相同形状的张量 - 就像我们的 `(2, 4) * (1, 4)`上面的示例返回形状为 `(2, 4)` 的张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "广播规则如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 每个张量必须至少有一个维度 - 没有空张量。\n",
    "- 比较两个张量的维度大小，从最后到第一个：\n",
    "    - 每个维度必须相等，或者\n",
    "    - 其中一个维度必须为 1，或者\n",
    "    - 该维度在张量之一中不存在"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，正如您之前所见，形状相同的张量通常是“可广播的”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是遵守上述规则并允许广播的一些情况示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6493, 0.2633],\n",
      "         [0.4762, 0.0548],\n",
      "         [0.2024, 0.5731]],\n",
      "\n",
      "        [[0.6493, 0.2633],\n",
      "         [0.4762, 0.0548],\n",
      "         [0.2024, 0.5731]],\n",
      "\n",
      "        [[0.6493, 0.2633],\n",
      "         [0.4762, 0.0548],\n",
      "         [0.2024, 0.5731]],\n",
      "\n",
      "        [[0.6493, 0.2633],\n",
      "         [0.4762, 0.0548],\n",
      "         [0.2024, 0.5731]]])\n",
      "tensor([[[0.7191, 0.7191],\n",
      "         [0.4067, 0.4067],\n",
      "         [0.7301, 0.7301]],\n",
      "\n",
      "        [[0.7191, 0.7191],\n",
      "         [0.4067, 0.4067],\n",
      "         [0.7301, 0.7301]],\n",
      "\n",
      "        [[0.7191, 0.7191],\n",
      "         [0.4067, 0.4067],\n",
      "         [0.7301, 0.7301]],\n",
      "\n",
      "        [[0.7191, 0.7191],\n",
      "         [0.4067, 0.4067],\n",
      "         [0.7301, 0.7301]]])\n",
      "tensor([[[0.6276, 0.7357],\n",
      "         [0.6276, 0.7357],\n",
      "         [0.6276, 0.7357]],\n",
      "\n",
      "        [[0.6276, 0.7357],\n",
      "         [0.6276, 0.7357],\n",
      "         [0.6276, 0.7357]],\n",
      "\n",
      "        [[0.6276, 0.7357],\n",
      "         [0.6276, 0.7357],\n",
      "         [0.6276, 0.7357]],\n",
      "\n",
      "        [[0.6276, 0.7357],\n",
      "         [0.6276, 0.7357],\n",
      "         [0.6276, 0.7357]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4, 3, 2)\n",
    "\n",
    "b = a * torch.rand(   3, 2) # 3rd & 2nd dims identical to a, dim 1 absent\n",
    "print(b)\n",
    "\n",
    "c = a * torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to a\n",
    "print(c)\n",
    "\n",
    "d = a * torch.rand(   1, 2) # 3rd dim identical to a, 2nd dim = 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仔细观察上面每个张量的值："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 创建`b`乘法运算在`a`的每个“层”上广播。\n",
    "- 对于`c` ，该操作在`a`的每一层和每一行上广播 - 每个 3 元素列都是相同的。\n",
    "- 对于`d` ，我们将其切换 - 现在跨层和列的每一行都是相同的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有关广播的更多信息，请参阅有关该主题的[PyTorch文档](https://pytorch.org/docs/stable/notes/broadcasting.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是一些尝试广播失败的示例："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### NOTE\n",
    "> 以下单元格抛出运行时错误。\n",
    "> ```python\n",
    "> a = torch.ones(4, 3, 2)\n",
    "> b = a * torch.rand(4, 3)    # dimensions must match last-to-first\n",
    "> c = a * torch.rand(   2, 3) # both 3rd & 2nd dims different\n",
    "> d = a * torch.rand((0, ))   # can't broadcast with an empty tensor\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Math with Tensors 更多关于张量的数学"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 张量有超过三百种可以对其执行的操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是一些主要操作类别的一个小样本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common functions:\n",
      "tensor([[0.9238, 0.5724, 0.0791, 0.2629],\n",
      "        [0.1986, 0.4439, 0.6434, 0.4776]])\n",
      "tensor([[-0., -0., 1., -0.],\n",
      "        [-0., 1., 1., -0.]])\n",
      "tensor([[-1., -1.,  0., -1.],\n",
      "        [-1.,  0.,  0., -1.]])\n",
      "tensor([[-0.5000, -0.5000,  0.0791, -0.2629],\n",
      "        [-0.1986,  0.4439,  0.5000, -0.4776]])\n",
      "\n",
      "Sine and arcsine:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 0.7854])\n",
      "\n",
      "Bitwise XOR:\n",
      "tensor([3, 2, 1])\n",
      "\n",
      "Broadcasted, element-wise equality comparison:\n",
      "tensor([[ True, False],\n",
      "        [False, False]])\n",
      "\n",
      "Reduction ops:\n",
      "tensor(4.)\n",
      "4.0\n",
      "tensor(2.5000)\n",
      "tensor(1.2910)\n",
      "tensor(24.)\n",
      "tensor([1, 2])\n",
      "\n",
      "Vectors & Matrices:\n",
      "tensor([ 0.,  0., -1.])\n",
      "tensor([[0.7375, 0.8328],\n",
      "        [0.8444, 0.2941]])\n",
      "tensor([[2.2125, 2.4985],\n",
      "        [2.5332, 0.8822]])\n",
      "torch.return_types.linalg_svd(\n",
      "U=tensor([[-0.7889, -0.6145],\n",
      "        [-0.6145,  0.7889]]),\n",
      "S=tensor([4.1498, 1.0548]),\n",
      "Vh=tensor([[-0.7957, -0.6056],\n",
      "        [ 0.6056, -0.7957]]))\n"
     ]
    }
   ],
   "source": [
    "# common functions\n",
    "a = torch.rand(2, 4) * 2 - 1\n",
    "print('Common functions:')\n",
    "print(torch.abs(a))\n",
    "print(torch.ceil(a))\n",
    "print(torch.floor(a))\n",
    "print(torch.clamp(a, -0.5, 0.5))\n",
    "\n",
    "# trigonometric functions and their inverses\n",
    "angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "sines = torch.sin(angles)\n",
    "inverses = torch.asin(sines)\n",
    "print('\\nSine and arcsine:')\n",
    "print(angles)\n",
    "print(sines)\n",
    "print(inverses)\n",
    "\n",
    "# bitwise operations\n",
    "print('\\nBitwise XOR:')\n",
    "b = torch.tensor([1, 5, 11])\n",
    "c = torch.tensor([2, 7, 10])\n",
    "print(torch.bitwise_xor(b, c))\n",
    "\n",
    "# comparisons:\n",
    "print('\\nBroadcasted, element-wise equality comparison:')\n",
    "d = torch.tensor([[1., 2.], [3., 4.]])\n",
    "e = torch.ones(1, 2)  # many comparison ops support broadcasting!\n",
    "print(torch.eq(d, e)) # returns a tensor of type bool\n",
    "\n",
    "# reductions:\n",
    "print('\\nReduction ops:')\n",
    "print(torch.max(d))        # returns a single-element tensor\n",
    "print(torch.max(d).item()) # extracts the value from the returned tensor\n",
    "print(torch.mean(d))       # average\n",
    "print(torch.std(d))        # standard deviation\n",
    "print(torch.prod(d))       # product of all numbers\n",
    "print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))) # filter unique elements\n",
    "\n",
    "# vector and linear algebra operations\n",
    "v1 = torch.tensor([1., 0., 0.])         # x unit vector\n",
    "v2 = torch.tensor([0., 1., 0.])         # y unit vector\n",
    "m1 = torch.rand(2, 2)                   # random matrix\n",
    "m2 = torch.tensor([[3., 0.], [0., 3.]]) # three times identity matrix\n",
    "\n",
    "print('\\nVectors & Matrices:')\n",
    "print(torch.linalg.cross(v2, v1)) # negative of z unit vector (v1 x v2 == -v2 x v1)\n",
    "print(m1)\n",
    "m3 = torch.linalg.matmul(m1, m2)\n",
    "print(m3)                  # 3 times m1\n",
    "print(torch.linalg.svd(m3))       # singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是操作的一个小样本。有关更多详细信息和数学函数的完整清单，请查看[文档](https://pytorch.org/docs/stable/torch.html#math-operations)。有关线性代数运算的更多详细信息和完整清单，请查看此[文档](https://pytorch.org/docs/stable/linalg.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Altering Tensors in Place 就地改变张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大多数张量上的二元运算都会返回第三个新张量。当我们说`c = a * b` （其中a和b是张量）时，新张量c将占据与其他张量不同的内存区域。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不过，有时您可能希望就地更改张量 - 例如，如果您正在进行逐元素计算，可以丢弃中间值。为此，大多数数学函数都有一个带有附加下划线（ `_` ）的版本，它将改变张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "\n",
      "b:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print('a:')\n",
    "print(a)\n",
    "print(torch.sin(a))   # this operation creates a new tensor in memory\n",
    "print(a)              # a has not changed\n",
    "\n",
    "b = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print('\\nb:')\n",
    "print(b)\n",
    "print(torch.sin_(b))  # note the underscore\n",
    "print(b)              # b has changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于算术运算，有一些行为类似的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.3788, 0.4567],\n",
      "        [0.0649, 0.6677]])\n",
      "\n",
      "After adding:\n",
      "tensor([[1.3788, 1.4567],\n",
      "        [1.0649, 1.6677]])\n",
      "tensor([[1.3788, 1.4567],\n",
      "        [1.0649, 1.6677]])\n",
      "tensor([[0.3788, 0.4567],\n",
      "        [0.0649, 0.6677]])\n",
      "\n",
      "After multiplying\n",
      "tensor([[0.1435, 0.2086],\n",
      "        [0.0042, 0.4459]])\n",
      "tensor([[0.1435, 0.2086],\n",
      "        [0.0042, 0.4459]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "\n",
    "print('Before:')\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter adding:')\n",
    "print(a.add_(b))\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter multiplying')\n",
    "print(b.mul_(b))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，这些就地算术函数是`torch.Tensor`对象上的方法，而不是像许多其他函数（例如`torch.sin()` ）一样附加到torch模块。正如您从`a.add_(b)`中看到的，调用张量是就地更改的张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有另一种选择可以将计算结果放入现有的分配张量中。到目前为止我们已经见过的许多方法和函数 - 包括创建方法！ - 有一个`out`参数，可让您指定一个张量来接收输出。如果`out`张量的形状和`dtype`正确，则无需新的内存分配即可发生这种情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0.3653, 0.8699],\n",
      "        [0.2364, 0.3604]])\n",
      "tensor([[0.0776, 0.4004],\n",
      "        [0.9877, 0.0352]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "c = torch.zeros(2, 2)\n",
    "old_id = id(c)\n",
    "\n",
    "print(c)\n",
    "d = torch.matmul(a, b, out=c)\n",
    "print(c)                # contents of c have changed\n",
    "\n",
    "assert c is d           # test c & d are same object, not just containing equal values\n",
    "assert id(c) == old_id  # make sure that our new c is the same object as the old one\n",
    "\n",
    "torch.rand(2, 2, out=c) # works for creation too!\n",
    "print(c)                # c has changed again\n",
    "assert id(c) == old_id  # still the same object!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying Tensors 复制张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与 Python 中的任何对象一样，将张量分配给变量会使该变量成为张量的标签，并且不会复制它。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1., 561.],\n",
      "        [  1.,   1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a\n",
    "\n",
    "a[0][1] = 561  # we change a...\n",
    "print(b)       # ...and b is also altered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`clone()`时需要注意一件重要的事情。**如果您的源张量启用了 autograd，那么克隆张量也将启用**。这将在 autograd 的视频中更深入地介绍，但如果您想要详细信息的简单版本，请继续。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在许多情况下，这就是您想要的。例如，如果您的模型在其`forward()`方法中具有多个计算路径，并且原始张量及其克隆都对模型的输出有贡献，那么为了启用模型学习，您需要为两个张量打开autograd。如果您的源张量启用了自动梯度（如果它是一组学习权重或从涉及权重的计算中派生的，通常会启用自动梯度），那么您将得到您想要的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一方面，如果您正在进行计算，其中原始张量及其克隆都不需要跟踪梯度，那么只要源张量关闭了 autograd，您就可以开始了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不过，还有第三种情况：假设您正在模型的`forward()`函数中执行计算，默认情况下，所有内容都打开梯度，但您想在中途提取一些值以生成一些指标。在这种情况下，您不希望源张量的克隆副本跟踪梯度 - 通过关闭 autograd 的历史记录跟踪可以提高性能。为此，您可以在源张量上使用`.detach()`方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0905, 0.4485],\n",
      "        [0.8740, 0.2526]], requires_grad=True)\n",
      "tensor([[0.0905, 0.4485],\n",
      "        [0.8740, 0.2526]], grad_fn=<CloneBackward0>)\n",
      "tensor([[0.0905, 0.4485],\n",
      "        [0.8740, 0.2526]])\n",
      "tensor([[0.0905, 0.4485],\n",
      "        [0.8740, 0.2526]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2, requires_grad=True) # turn on autograd\n",
    "print(a)\n",
    "\n",
    "b = a.clone()\n",
    "print(b)\n",
    "\n",
    "c = a.detach().clone()\n",
    "print(c)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里发生了什么事？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们创建`a`打开的`requires_grad=True` 。我们还没有讨论这个可选参数，但会在 autograd 单元中讨论。\n",
    "- 当我们打印a时，它通知我们属性`requires_grad=True` - 这意味着自动梯度和计算历史跟踪已打开。\n",
    "- 我们克隆`a`并将其标记为`b` 。当我们打印`b`时，我们可以看到它正在跟踪其计算历史记录 - 它继承了`a`的 `autograd` 设置，并添加到了计算历史记录中。\n",
    "- 我们克隆 `a` 到 `c` ，但我们先调用 `detach()`。\n",
    "- 打印`c` ，我们没有看到计算历史记录，也没有`requires_grad=True`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`detach()`方法将张量从其计算历史中分离出来。它说，“接下来的任何事情关闭自动梯度。”它在不更改`a`情况下执行此操作 - 您可以看到，当我们在末尾再次打印`a`时，它保留了其`requires_grad=True`属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to GPU 移动到 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 的主要优势之一是它在兼容 CUDA 的 Nvidia GPU 上具有强大的加速能力。 （“CUDA”代表统一计算设备架构，它是 Nvidia 的并行计算平台。）到目前为止，我们所做的一切都是在 CPU 上进行的。我们如何转向更快的硬件？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有多种方法可以将数据传输到目标设备上。您可以在创建时执行此操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2172, 0.3683],\n",
      "        [0.0173, 0.0119]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_rand = torch.rand(2, 2, device='cuda')\n",
    "    print(gpu_rand)\n",
    "elif torch.backends.mps.is_available():\n",
    "    mps_rand = torch.rand(2, 2, device='mps')\n",
    "    print(mps_rand)\n",
    "else:\n",
    "    cpu_rand = torch.rand(2, 2)\n",
    "    print(cpu_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下，新的张量是在 CPU 上创建的，因此我们必须使用可选的`device`参数指定何时在 GPU 上创建张量。您可以看到，当我们打印新张量时，PyTorch 会通知我们它位于哪个设备上（如果它不在 CPU 上）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以使用`torch.cuda.device_count()`查询 GPU 的数量。如果您有多个 GPU，则可以通过索引指定它们： `device='cuda:0'` 、 `device='cuda:1'`等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作为一种编码实践，用字符串常量指定我们的设备是非常脆弱的。在理想的情况下，无论您使用的是 CPU 还是 GPU 硬件，您的代码都可以稳定运行。您可以通过创建一个可以传递给张量而不是字符串的设备句柄来做到这一点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "tensor([[0.4699, 0.1754],\n",
      "        [0.7270, 0.4311]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    my_device = torch.device('mps')\n",
    "else:\n",
    "    my_device = torch.device('cpu')\n",
    "print('Device: {}'.format(my_device))\n",
    "\n",
    "x = torch.rand(2, 2, device=my_device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果一台设备上有一个现有张量，则可以使用`to()`方法将其移动到另一台设备上。以下代码行在 CPU 上创建一个张量，并将其移动到您在上一个单元中获取的设备句柄。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(2, 2)\n",
    "y = y.to(my_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重要的是要知道，为了进行涉及两个或多个张量的计算，所有张量必须位于同一设备上。无论您是否有可用的 GPU 设备，以下代码都会引发运行时错误："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### NOTE\n",
    "> ```python\n",
    "> x = torch.rand(2, 2)\n",
    "> y = torch.rand(2, 2, device='gpu')\n",
    "> z = x + y  # exception will be thrown\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Tensor Shapes 操纵张量形状"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有时，您需要更改张量的形状。下面，我们将了解一些常见情况以及如何处理它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the Number of Dimensions 更改维度数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可能需要更改维度数的一种情况是将单个输入实例传递给模型。 PyTorch 模型通常需要批量输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如，假设有一个模型适用于 `3 x 226 x 226` 图像 - 具有 3 个颜色通道的 226 像素正方形。当您加载并转换它时，您将获得形状为`(3, 226, 226)`的张量。不过，您的模型需要输入形状`(N, 3, 226, 226)` ，其中N是批次中的图像数量。那么如何制作一批呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 226, 226])\n",
      "torch.Size([1, 3, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 226, 226)\n",
    "b = a.unsqueeze(0)\n",
    "\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们利用了这样一个事实：范围 1 的任何维度都不会改变张量中的元素数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[0.2347]]]]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.rand(1, 1, 1, 1, 1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "继续上面的示例，假设模型的输出是每个输入的 20 元素向量。然后，您期望输出具有形状`(N, 20)` ，其中N是输入批次中的实例数。这意味着对于我们的单输入批次，我们将获得形状`(1, 20)`的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您想使用该输出进行一些非批处理计算（只需要 20 个元素向量）怎么办？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n",
      "tensor([[0.1899, 0.4067, 0.1519, 0.1506, 0.9585, 0.7756, 0.8973, 0.4929, 0.2367,\n",
      "         0.8194, 0.4509, 0.2690, 0.8381, 0.8207, 0.6818, 0.5057, 0.9335, 0.9769,\n",
      "         0.2792, 0.3277]])\n",
      "torch.Size([20])\n",
      "tensor([0.1899, 0.4067, 0.1519, 0.1506, 0.9585, 0.7756, 0.8973, 0.4929, 0.2367,\n",
      "        0.8194, 0.4509, 0.2690, 0.8381, 0.8207, 0.6818, 0.5057, 0.9335, 0.9769,\n",
      "        0.2792, 0.3277])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1, 20)\n",
    "print(a.shape)\n",
    "print(a)\n",
    "\n",
    "b = a.squeeze(0)\n",
    "print(b.shape)\n",
    "print(b)\n",
    "\n",
    "c = torch.rand(2, 2)\n",
    "print(c.shape)\n",
    "\n",
    "d = c.squeeze(0)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以从形状中看到我们的二维张量现在是一维的，如果您仔细观察上面单元格的输出，您会发现打印`a`显示了一组“额外”的方括号`[]`因为具有额外的维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您只能`squeeze()`范围为 `1` 的维度。请参阅上面我们尝试在`c`中挤压大小为 `2` 的维度，并返回与我们开始时相同的形状。对`squeeze()`和`unsqueeze()`调用只能作用于范围为1 的维度，因为否则会改变张量中的元素数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可能使用`unsqueeze()`另一个地方是简化广播。回想一下上面的例子，我们有以下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1891, 0.1891],\n",
      "         [0.3952, 0.3952],\n",
      "         [0.9176, 0.9176]],\n",
      "\n",
      "        [[0.1891, 0.1891],\n",
      "         [0.3952, 0.3952],\n",
      "         [0.9176, 0.9176]],\n",
      "\n",
      "        [[0.1891, 0.1891],\n",
      "         [0.3952, 0.3952],\n",
      "         [0.9176, 0.9176]],\n",
      "\n",
      "        [[0.1891, 0.1891],\n",
      "         [0.3952, 0.3952],\n",
      "         [0.9176, 0.9176]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4, 3, 2)\n",
    "\n",
    "c = a * torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to a\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其最终效果是在维度 `0` 和 `2` 上广播操作，导致随机 `3 x 1` 张量按元素`a`中的每个 3 元素列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果随机向量只是三元素向量怎么办？我们将失去进行广播的能力，因为最终尺寸将不符合广播规则。 `unsqueeze()`来拯救："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "tensor([[[0.8960, 0.8960],\n",
      "         [0.4887, 0.4887],\n",
      "         [0.8625, 0.8625]],\n",
      "\n",
      "        [[0.8960, 0.8960],\n",
      "         [0.4887, 0.4887],\n",
      "         [0.8625, 0.8625]],\n",
      "\n",
      "        [[0.8960, 0.8960],\n",
      "         [0.4887, 0.4887],\n",
      "         [0.8625, 0.8625]],\n",
      "\n",
      "        [[0.8960, 0.8960],\n",
      "         [0.4887, 0.4887],\n",
      "         [0.8625, 0.8625]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4, 3, 2)\n",
    "b = torch.rand(   3)     # trying to multiply a * b will give a runtime error\n",
    "c = b.unsqueeze(1)       # change to a 2-dimensional tensor, adding new dim at the end\n",
    "print(c.shape)\n",
    "print(a * c)             # broadcasting works again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这 `squeeze()` 和 `unsqueeze()` 方法也有就地版本， `squeeze_()` 和 `unsqueeze_()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 226, 226])\n",
      "torch.Size([1, 3, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "batch_me = torch.rand(3, 226, 226)\n",
    "print(batch_me.shape)\n",
    "batch_me.unsqueeze_(0)\n",
    "print(batch_me.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有时您会想要更彻底地改变张量的形状，同时仍然保留元素的数量及其内容。发生这种情况的一种情况是在模型的卷积层和模型的线性层之间的接口处 - 这在图像分类模型中很常见。卷积核将产生形状特征 `x` 宽度 `x` 高度的输出张量，但下面的线性层需要一维输入。 `reshape()`将为您执行此操作，前提是您请求的维度产生与输入张量相同数量的元素："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 20, 20])\n",
      "torch.Size([2400])\n",
      "torch.Size([6, 20, 20])\n",
      "tensor(0.6973)\n",
      "tensor(512.)\n",
      "torch.Size([2400])\n"
     ]
    }
   ],
   "source": [
    "output3d = torch.rand(6, 20, 20)\n",
    "print(output3d.shape)\n",
    "\n",
    "input1d = output3d.reshape(6 * 20 * 20)\n",
    "print(input1d.shape)\n",
    "print(output3d.shape)\n",
    "\n",
    "print(input1d[0])\n",
    "output3d[0, 0, 0] = 512\n",
    "print(input1d[0])\n",
    "\n",
    "# can also call it as a method on the torch module:\n",
    "print(torch.reshape(output3d, (6 * 20 * 20,)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### NOTE\n",
    "> 上面单元格最后一行中的`(6 * 20 * 20,)`参数是因为 PyTorch 在指定张量形状时需要一个`元组`- 但是当形状是方法的第一个参数时，我们必须添加括号和逗号来让方法相信这确实是一个单元素元组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果可以， `reshape()`将返回要更改的张量的视图- 即查看同一底层内存区域的单独张量对象。这很重要：这意味着对源张量所做的任何更改都将反映在该张量的视图中，除非您`clone()`它。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在某些情况下， reshape()必须返回携带数据副本的张量，这超出了本介绍的范围。有关更多信息，请参阅[文档](https://pytorch.org/docs/stable/torch.html#torch.reshape)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Bridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面关于广播的部分中，提到 PyTorch 的广播语义与 NumPy 兼容 - 但 PyTorch 和 NumPy 之间的亲缘关系比这更深。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您现有的 ML 或科学代码的数据存储在 NumPy ndarray 中，您可能希望将相同的数据表示为 PyTorch 张量，无论是利用 PyTorch 的 GPU 加速还是其构建 ML 模型的高效抽象。在 ndarrays 和 PyTorch 张量之间切换很容易："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numpy_array = np.ones((2, 3))\n",
    "print(numpy_array)\n",
    "\n",
    "pytorch_tensor = torch.from_numpy(numpy_array)\n",
    "print(pytorch_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 创建一个与 NumPy 数组形状相同并包含相同数据的张量，甚至保留了 NumPy 的默认 64 位浮点数据类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换也可以很容易地以另一种方式进行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2853, 0.9091, 0.5695],\n",
      "        [0.7206, 0.4155, 0.0982]])\n",
      "[[0.2853077  0.90905803 0.5695162 ]\n",
      " [0.7206341  0.41554475 0.09820974]]\n"
     ]
    }
   ],
   "source": [
    "pytorch_rand = torch.rand(2, 3)\n",
    "print(pytorch_rand)\n",
    "\n",
    "numpy_rand = pytorch_rand.numpy()\n",
    "print(numpy_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重要的是要知道这些转换后的对象使用与其源对象相同的底层内存，这意味着对一个对象的更改会反映在另一个对象中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1., 23.,  1.]], dtype=torch.float64)\n",
      "[[ 0.2853077   0.90905803  0.5695162 ]\n",
      " [ 0.7206341  17.          0.09820974]]\n"
     ]
    }
   ],
   "source": [
    "numpy_array[1, 1] = 23\n",
    "print(pytorch_tensor)\n",
    "\n",
    "pytorch_rand[1, 1] = 17\n",
    "print(numpy_rand)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
